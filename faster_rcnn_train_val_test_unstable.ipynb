{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a9c04c-03f1-429d-b6ae-93171dc88cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0270f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "from torchvision.ops import nms\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import copy\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc882db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set direction and set parameters\n",
    "data_dir = './Data'\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "label_dir = os.path.join(data_dir, 'xmls')\n",
    "use_subset = False\n",
    "sub_percentage = 0.1\n",
    "train_batch_size = 30\n",
    "test_batch_size = 1\n",
    "step_size = 50\n",
    "gamma = 0.001\n",
    "lr = 0.001\n",
    "nms_step_size = 0.1\n",
    "nms_step = 3\n",
    "weight_decay= 0.001\n",
    "num_epochs = 30\n",
    "nms_iou_thresh = 0.4\n",
    "mAP_iou_threshold= 0.5\n",
    "score_thresh_init = 0.0\n",
    "num_classes = 5\n",
    "local_model_path = './faster_rcnn_model.pth'\n",
    "patience = 5\n",
    "num_worker = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99eeb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # set direction and set parameters\n",
    "# data_dir = './Data'\n",
    "# image_dir = os.path.join(data_dir, 'images')\n",
    "# label_dir = os.path.join(data_dir, 'xmls')\n",
    "# use_subset = True\n",
    "# sub_percentage = 0.1\n",
    "# train_batch_size = 4\n",
    "# test_batch_size = 1\n",
    "# step_size = 20\n",
    "# gamma = 0.005\n",
    "# lr = 0.005\n",
    "# weight_decay= 0.001\n",
    "# num_epochs = 10\n",
    "# nms_iou_thresh = 0.01\n",
    "# mAP_iou_threshold = 0.5\n",
    "# score_thresh = 0.4\n",
    "# num_classes = 5\n",
    "# local_model_path = './faster_rcnn_model.pth'\n",
    "# patience = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e21b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset class\n",
    "class RDD2022Dataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.classes = ['Background', 'D00', 'D10', 'D20', 'D40']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        print(self.class_to_idx)\n",
    "\n",
    "        self.images = list(sorted(os.listdir(image_dir)))\n",
    "        self.labels = list(sorted(os.listdir(label_dir)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load image and label\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label_path = os.path.join(self.label_dir, self.labels[idx])\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.findtext('name')\n",
    "            labels.append(self.class_to_idx[cls])\n",
    "            xmlbox = obj.find('bndbox')\n",
    "            bbox_coords = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            boxes.append([int(float(xmlbox.findtext(tag))) for tag in bbox_coords])\n",
    "        # if have label\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # if it doesn't have label\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            area = torch.zeros(0, dtype=torch.float32)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        iscrowd = torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1697fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Background': 0, 'D00': 1, 'D10': 2, 'D20': 3, 'D40': 4}\n",
      "{'Background': 0, 'D00': 1, 'D10': 2, 'D20': 3, 'D40': 4}\n",
      "Using full dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_transform():\n",
    "    def transform(img, target):\n",
    "        img = F.to_tensor(img)\n",
    "        return img, target\n",
    "    return transform\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = RDD2022Dataset(\n",
    "    os.path.join(image_dir, 'train'),\n",
    "    os.path.join(label_dir, 'train'),\n",
    "    transforms=get_transform()\n",
    ")\n",
    "\n",
    "val_dataset = RDD2022Dataset(\n",
    "    os.path.join(image_dir, 'val'),\n",
    "    os.path.join(label_dir, 'val'),\n",
    "    transforms=get_transform()\n",
    ")\n",
    "\n",
    "def create_subset(dataset, subset_size):\n",
    "    # Check subset condition\n",
    "    if isinstance(subset_size, float) and subset_size > 0 and subset_size < 1:\n",
    "        subset_size = int(len(dataset) * subset_size)\n",
    "    elif isinstance(subset_size, int) and subset_size > 0 and subset_size < len(dataset):\n",
    "        subset_size = subset_size\n",
    "    else:\n",
    "        raise ValueError(\"subset_size must be a positive integer or a float between 0 and 1.\")\n",
    "    # Generate a random subset\n",
    "    indices = torch.randperm(len(dataset))[:subset_size]\n",
    "    subset = Subset(dataset, indices)\n",
    "    return subset\n",
    "\n",
    "# DataLoader setup\n",
    "# if using subset\n",
    "if use_subset:\n",
    "    train_subset_size = sub_percentage \n",
    "    val_subset_size = sub_percentage\n",
    "    train_subset = create_subset(train_dataset, train_subset_size)\n",
    "    val_subset = create_subset(val_dataset, val_subset_size)\n",
    "    train_loader = DataLoader(train_subset, batch_size=train_batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loader = DataLoader(val_subset, batch_size=train_batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    print('Using subset')\n",
    "# if using full dataset\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    print('Using full dataset')\n",
    "\n",
    "# Clear CUDA cache\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2abff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate Intersection over Union (IoU)\n",
    "def calculate_iou(gt_box, pred_box):\n",
    "    xi1 = max(gt_box[0], pred_box[0])\n",
    "    yi1 = max(gt_box[1], pred_box[1])\n",
    "    xi2 = min(gt_box[2], pred_box[2])\n",
    "    yi2 = min(gt_box[3], pred_box[3])\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    \n",
    "    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "    union_area = gt_area + pred_area - inter_area\n",
    "    \n",
    "    if union_area == 0:\n",
    "        return 0\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# apply Non-Max Suppression (NMS) to filter predictions\n",
    "def apply_nms(orig_prediction, epoch, iou_thresh=nms_iou_thresh, score_thresh_init=score_thresh_init, nms_step = nms_step, step_size = nms_step_size):\n",
    "    # if no detections to process\n",
    "    if orig_prediction['scores'].nelement() == 0:\n",
    "        return {\n",
    "            'boxes': torch.empty((0, 4), dtype=torch.float32),\n",
    "            'scores': torch.empty((0,), dtype=torch.float32),\n",
    "            'labels': torch.empty((0,), dtype=torch.int64)\n",
    "        }\n",
    "    # apply NMS\n",
    "    boxes = orig_prediction['boxes'].float()\n",
    "    scores = orig_prediction['scores'].float()\n",
    "    if epoch//nms_step <= 5:\n",
    "        score_thresh = score_thresh_init + (epoch//nms_step)*nms_step_size\n",
    "    else:\n",
    "        score_thresh = 0.5\n",
    "    high_score_mask = scores > score_thresh\n",
    "    boxes = boxes[high_score_mask]\n",
    "    scores = scores[high_score_mask]\n",
    "    labels = orig_prediction['labels'][high_score_mask]\n",
    "    keep = nms(boxes, scores, iou_thresh)\n",
    "    return {'boxes': boxes[keep], 'scores': scores[keep],'labels': labels[keep]}\n",
    "\n",
    "# update true positives, false positives, and false negatives for each class\n",
    "def compute_tp_fp_fn_for_image(epoch_index, gt_boxes, gt_labels, pred_boxes, pred_scores, pred_labels, num_classes, iou_threshold= mAP_iou_threshold):\n",
    "    # iterate through each class (ignoring the background class)\n",
    "    for cls in range(1, num_classes):\n",
    "        # filter ground truth and predictions for the current class\n",
    "        gt_idx = np.where(gt_labels == cls)[0]\n",
    "        pred_idx = np.where(pred_labels == cls)[0]\n",
    "\n",
    "        if len(gt_idx) == 0 and len(pred_idx) == 0:\n",
    "            continue\n",
    "        elif len(gt_idx) == 0:\n",
    "            epoch_index[cls-1,1] += len(pred_boxes[pred_idx])\n",
    "            continue\n",
    "        elif len(pred_idx) == 0:\n",
    "            epoch_index[cls-1,2] += len(gt_boxes[gt_idx])\n",
    "            continue\n",
    "\n",
    "        class_gt_boxes = gt_boxes[gt_idx]\n",
    "        class_pred_boxes = pred_boxes[pred_idx]\n",
    "        class_pred_scores = pred_scores[pred_idx]\n",
    "\n",
    "        # sort predictions by scores in descending order\n",
    "        sorted_indices = np.argsort(-class_pred_scores)\n",
    "        class_pred_boxes = class_pred_boxes[sorted_indices]\n",
    "\n",
    "        # initialize counters for true positives, false positives, and false negatives\n",
    "        tp = 0\n",
    "        detected = np.zeros(len(class_gt_boxes), dtype=bool)\n",
    "\n",
    "        # check each predicted box against all GT boxes of the same class\n",
    "        for pred_box in class_pred_boxes:\n",
    "            ious = [calculate_iou(pred_box, gt_box) for gt_box in class_gt_boxes]\n",
    "            max_iou = max(ious)\n",
    "            max_gt_idx = np.argmax(ious)\n",
    "\n",
    "            if max_iou >= iou_threshold and not detected[max_gt_idx]:\n",
    "                tp += 1\n",
    "                detected[max_gt_idx] = True\n",
    "\n",
    "        # update class metrics\n",
    "        epoch_index[cls-1,0] += tp\n",
    "        epoch_index[cls-1,1] += (len(class_pred_boxes) - tp)  # All non-TPs are FPs\n",
    "        epoch_index[cls-1,2] += (len(class_gt_boxes) - np.sum(detected))  # GTs not detected are FNs\n",
    "\n",
    "    return epoch_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6bbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model and calculate mAP along with precision, recall, and F1 scores\n",
    "def evaluate_or_test_model(model, data_loader, device, epoch):\n",
    "    model.eval()\n",
    "    # total_val_loss = 0\n",
    "    epoch_index = np.zeros((num_classes-1, 3))\n",
    "    all_detections = [[] for _ in range(num_classes-1)]\n",
    "    all_annotations = [[] for _ in range(num_classes-1)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # for each evaluation batch\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            for i, output in enumerate(outputs):\n",
    "                nms_output = apply_nms(output, epoch)\n",
    "                pred_boxes = nms_output['boxes'].cpu().numpy()\n",
    "                pred_labels = nms_output['labels'].cpu().numpy()\n",
    "                pred_scores = nms_output['scores'].cpu().numpy()\n",
    "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "                gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "                epoch_index = compute_tp_fp_fn_for_image(epoch_index, gt_boxes, gt_labels, pred_boxes, pred_scores, pred_labels, num_classes)\n",
    "\n",
    "                # Store detections and annotations for mAP calculation\n",
    "                for label in range(1, num_classes):\n",
    "                    pred_indices = np.where(pred_labels == label)[0]\n",
    "                    gt_indices = np.where(gt_labels == label)[0]\n",
    "                    all_detections[label-1].extend([(box, score) for box, score in zip(pred_boxes[pred_indices], pred_scores[pred_indices])])\n",
    "                    all_annotations[label-1].extend(gt_boxes[gt_indices])\n",
    "\n",
    "            # Compute losses\n",
    "            # if targets:\n",
    "            #     model.train()\n",
    "            #     loss_dict = model(images, targets)\n",
    "            #     losses = sum(loss for loss in loss_dict.values())\n",
    "            #     total_val_loss += losses.item()\n",
    "            #     model.eval()\n",
    "\n",
    "            del images, targets, outputs\n",
    "            clear_memory()\n",
    "\n",
    "    # avg_loss = total_val_loss / len(data_loader)\n",
    "\n",
    "    # calculate mAP for each class\n",
    "    avg_precisions = []\n",
    "    for class_detections, class_annotations in zip(all_detections, all_annotations):\n",
    "        if not class_detections or not class_annotations:\n",
    "            continue\n",
    "        # sort detections by decreasing confidence\n",
    "        sorted_detections = sorted(class_detections, key=lambda x: x[1], reverse=True)\n",
    "        tp = np.zeros(len(sorted_detections))\n",
    "        fp = np.zeros(len(sorted_detections))\n",
    "        matched = np.zeros(len(class_annotations), dtype=bool)  # Tracks which GT boxes have been matched\n",
    "\n",
    "        for d_idx, (bbox, score) in enumerate(sorted_detections):\n",
    "            ious = [calculate_iou(bbox, gt_bbox) for gt_bbox in class_annotations]\n",
    "            best_iou = max(ious) if ious else 0\n",
    "            best_idx = np.argmax(ious) if ious else -1\n",
    "\n",
    "            if best_iou >= mAP_iou_threshold:\n",
    "                if not matched[best_idx]:\n",
    "                    tp[d_idx] = 1\n",
    "                    matched[best_idx] = True\n",
    "                else:\n",
    "                    fp[d_idx] = 1\n",
    "            else:\n",
    "                fp[d_idx] = 1\n",
    "\n",
    "        # accumulate true positives and false positives for precision-recall calculation\n",
    "        acc_tp = np.cumsum(tp)\n",
    "        acc_fp = np.cumsum(fp)\n",
    "        recall = acc_tp / len(class_annotations) if len(class_annotations) > 0 else np.zeros_like(acc_tp)\n",
    "        precision = acc_tp / (acc_tp + acc_fp) if (acc_tp + acc_fp).any() else np.zeros_like(acc_tp)\n",
    "\n",
    "        # calculate area under the precision-recall curve (AUC)\n",
    "        ap = auc(recall, precision)\n",
    "        avg_precisions.append(ap)\n",
    "\n",
    "    # compute the mean of average precisions across all classes\n",
    "    mAP50 = np.mean(avg_precisions) if avg_precisions else 0\n",
    "\n",
    "    # calculate overall precision, recall, and F1 score\n",
    "    tp_total = epoch_index[:, 0]\n",
    "    fp_total = epoch_index[:, 1]\n",
    "    fn_total = epoch_index[:, 2]\n",
    "    recall = tp_total / (tp_total + fn_total + 1e-6)\n",
    "    precision = tp_total / (tp_total + fp_total + 1e-6)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "    # return avg_loss, mean_ap, precision, recall, f1_score\n",
    "\n",
    "    return mAP50, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.4199, mAP50: 0.3865\n",
      "F1 per group: [0.17325309 0.12802071 0.12346902 0.08570451], Recall per group: [0.63466397 0.59855636 0.61901763 0.13477366], Precision per group: [0.10031949 0.07167553 0.06857342 0.06282974]\n",
      "General F1: 0.12761183101361503, General Recall: 0.4967529057635237, General Precision: 0.07584954464881952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 0.3633, mAP50: 0.4569\n",
      "F1 per group: [0.20842904 0.19999974 0.21292906 0.08395783], Recall per group: [0.6859525  0.66240977 0.64609572 0.40329218], Precision per group: [0.12288404 0.11778063 0.12746925 0.04685632]\n",
      "General F1: 0.17632891879578327, General Recall: 0.5994375428007923, General Precision: 0.10374756120092359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 0.3478, mAP50: 0.4882\n",
      "F1 per group: [0.16117423 0.17847894 0.21714498 0.09501495], Recall per group: [0.72536635 0.70960577 0.65239295 0.50102881], Precision per group: [0.09065934 0.10207668 0.13024893 0.0524841 ]\n",
      "General F1: 0.16295327152375683, General Recall: 0.647098468348369, General Precision: 0.0938672632970964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 0.3391, mAP50: 0.4922\n",
      "F1 per group: [0.28117905 0.27327926 0.30367071 0.19088462], Recall per group: [0.69151086 0.68684064 0.6511335  0.48045267], Precision per group: [0.1764668  0.17057363 0.19800843 0.11910227]\n",
      "General F1: 0.2622534094499187, General Recall: 0.6274844207138857, General Precision: 0.16603778152286935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 0.3334, mAP50: 0.5074\n",
      "F1 per group: [0.30814881 0.26743788 0.30989475 0.17326212], Recall per group: [0.6816574  0.70571904 0.68828715 0.53600823], Precision per group: [0.19907032 0.16497923 0.19996341 0.10333201]\n",
      "General F1: 0.26468588984973795, General Recall: 0.6529179575649455, General Precision: 0.16683624232214916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.3270, mAP50: 0.5127\n",
      "F1 per group: [0.34663868 0.32972057 0.3492881  0.21644803], Recall per group: [0.676857   0.6757357  0.68010076 0.51851852], Precision per group: [0.23297678 0.21806128 0.23498695 0.13677069]\n",
      "General F1: 0.31052384688284923, General Recall: 0.6378029933874323, General Precision: 0.20569892423673625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 0.3221, mAP50: 0.4983\n",
      "F1 per group: [0.41209363 0.37876346 0.47259906 0.30703441], Recall per group: [0.65083375 0.66352027 0.63539043 0.48045267], Precision per group: [0.30149813 0.2650255  0.37621178 0.22560386]\n",
      "General F1: 0.39262264141570846, General Recall: 0.6075492806553666, General Precision: 0.292084819632578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:  34%|███▍      | 190/555 [12:07<23:51,  3.92s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, patience=patience):\n",
    "    best_mAP50 = 0\n",
    "    last_mAP50 = 0\n",
    "    best_model_wts = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "            model.train()\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_train_loss += losses.item()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            del images, targets, loss_dict \n",
    "            clear_memory()\n",
    "        mAP50, precision, recall, f1_score = evaluate_or_test_model(model, val_loader, device, epoch)\n",
    "        # print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss/len(train_loader):.4f}, '\n",
    "        #       f'Val Loss: {val_loss:.4f}, mAP: {mean_ap:.4f}')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss/len(train_loader):.4f}, mAP50: {mAP50:.4f}')\n",
    "        print(f'F1 per group: {f1_score}, Recall per group: {recall}, Precision per group: {precision}')\n",
    "        print(f'General F1: {sum(f1_score) / len(f1_score)}, General Recall: {sum(recall) / len(recall)}, General Precision: {sum(precision) / len(precision)}')\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        \n",
    "        if mAP50 >= best_mAP50:\n",
    "            best_mAP50 = mAP50\n",
    "            last_mAP50 = mAP50\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        elif mAP50 >= last_mAP50:\n",
    "            last_mAP50 = mAP50\n",
    "            epochs_no_improve -= 1\n",
    "        else:\n",
    "            last_mAP50 = mAP50\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "        clear_memory()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_mAP50\n",
    "\n",
    "# Initialize and train the model\n",
    "model = get_model(len(train_dataset.classes))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "trained_model, best_mAP50 = train_model(model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "save_model_path = f'./faster_rcnn_lr{lr}_bs{train_batch_size}_epochs{num_epochs}_mAP{best_mAP50}.pth'\n",
    "# Example usage:\n",
    "model_save_path = save_model_path\n",
    "save_model(trained_model, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison_three_plots(image_path, label_path, model, device, dataset_classes, transform=None):\n",
    "    # Load and prepare image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    if transform:\n",
    "        image_tensor, _ = transform(image, {})  # Assuming the transform function returns a tensor and a target\n",
    "    else:\n",
    "        image_tensor = F.to_tensor(image)  # Convert to tensor without any additional transformation\n",
    "\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension and transfer to device\n",
    "\n",
    "    # Load true labels and boxes\n",
    "    tree = ET.parse(label_path)\n",
    "    root = tree.getroot()\n",
    "    true_boxes = []\n",
    "    true_labels = []\n",
    "    for obj in root.iter('object'):\n",
    "        cls = obj.find('name').text\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        xmin = int(float(xmlbox.find('xmin').text))\n",
    "        ymin = int(float(xmlbox.find('ymin').text))\n",
    "        xmax = int(float(xmlbox.find('xmax').text))\n",
    "        ymax = int(float(xmlbox.find('ymax').text))\n",
    "        true_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        true_labels.append(dataset_classes.index(cls))\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    predictions = apply_nms(predictions[0], 30)  # Assuming apply_nms is defined correctly\n",
    "\n",
    "    # Prepare image for plotting\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Set up three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 18))\n",
    "    ax1.imshow(image_np)\n",
    "    ax2.imshow(image_np)\n",
    "    ax3.imshow(image_np)\n",
    "\n",
    "    # Draw predicted boxes and labels on the second subplot\n",
    "    pred_boxes = predictions['boxes'].cpu().numpy()\n",
    "    pred_scores = predictions['scores'].cpu().numpy()\n",
    "    pred_labels = predictions['labels'].cpu().numpy()\n",
    "\n",
    "    for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x1, y1, f'Pred: {dataset_classes[label]} {score:.2f}', color='white', fontsize=12,\n",
    "                 bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    # Draw true boxes and labels on the third subplot\n",
    "    for box, label in zip(true_boxes, true_labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax3.add_patch(rect)\n",
    "        ax3.text(x1, y2, f'True: {dataset_classes[label]}', color='white', fontsize=12,\n",
    "                 bbox=dict(facecolor='green', alpha=0.5))\n",
    "\n",
    "    ax1.set_title('Original Image')\n",
    "    ax2.set_title('Predictions')\n",
    "    ax3.set_title('Ground Truth')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = './Data/images/val/Norway_000830.jpg'\n",
    "label_path = './Data/xmls/val/Norway_000830.xml'\n",
    "visualize_comparison_three_plots(image_path, label_path, model, device, train_dataset.classes, transform=get_transform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './Data/images/val/China_Drone_000086.jpg'\n",
    "label_path = './Data/xmls/val/China_Drone_000086.xml'\n",
    "visualize_comparison_three_plots(image_path, label_path, model, device, train_dataset.classes, transform=get_transform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2959ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './Data/images/val/Japan_003127.jpg'\n",
    "label_path = './Data/xmls/val/Japan_003127.xml'\n",
    "visualize_comparison_three_plots(image_path, label_path, model, device, train_dataset.classes, transform=get_transform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "local_model_path = f'./faster_rcnn_lr{lr}_bs{train_batch_size}_epochs{num_epochs}_mAP{best_mAP50}.pth'\n",
    "test_image_dir = os.path.join(image_dir, 'test')\n",
    "test_label_dir = os.path.join(label_dir, 'test')\n",
    "model_path = local_model_path\n",
    "test_batch_size = 4\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = list(sorted(os.listdir(image_dir)))\n",
    "        self.labels = list(sorted(os.listdir(label_dir)))\n",
    "        self.class_to_idx = {'Background': 0, 'D00': 1, 'D10': 2, 'D20': 3, 'D40': 4}  # Update as necessary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.labels[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.findtext('name')\n",
    "            labels.append(self.class_to_idx[cls])\n",
    "            xmlbox = obj.find('bndbox')\n",
    "            boxes.append([int(xmlbox.findtext(tag)) for tag in ['xmin', 'ymin', 'xmax', 'ymax']])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if boxes.nelement() != 0 else torch.zeros(0, dtype=torch.float32)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([idx]), 'area': area, 'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)}\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def get_transform():\n",
    "    def transform(img):\n",
    "        return F.to_tensor(img)\n",
    "    return transform\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_path, num_classes):\n",
    "    model = get_model(num_classes)  # Ensure get_model is defined or imported\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "test_dataset = TestDataset(test_image_dir, test_label_dir, transforms=get_transform())\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "model = load_model(model_path, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mean_ap, precision, recall, f1_score = evaluate_or_test_model(model, test_loader, device, 1)\n",
    "print(f'Test mAP50: {mean_ap:.4f}')\n",
    "print(f'F1 per group: {f1_score}, Recall per group: {recall}, Precision per group: {precision}')\n",
    "print(f'General F1: {sum(f1_score) / len(f1_score)}, General Recall: {sum(recall) / len(recall)}, General Precision: {sum(precision) / len(precision)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
